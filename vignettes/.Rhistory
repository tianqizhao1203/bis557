cor(train[,-1])
wd <- '/Users/CindyZhao/Desktop/'
setwd(wd)
train1 <- read.csv("kc_house_data.csv",header=TRUE)
head(train1)
train <- train1[-1,]
train <- train1[-c(1,2),]
head(train)
train1 <- read.csv("kc_house_data.csv",header=TRUE)
train1 <- read.csv("kc_house_data.csv",header=TRUE)
train <- train1[,-c(1,2)]
cor(train)
train
cor(train[,-1])
### First simulate data based on y=sin(4x)+e.
### Bandwidth is set at 0.2.
### Construct b matrix [b(x)=t(1,x)] and X matrix [X = t(b(X1),b(X2), ..., b(Xn))]
### Calculate W matrix [diagonal matrix with diagonal vector t(Kh(X1-x0), ..., Kh(Xn-x0))]
### Calculate gamma matrix [(inverse(t(X)WX))*t(X)*W*t(Y1, ..., Yn)]
### Calculate fitted function [m(x)=t(b)*gamma]
### After obtained the fitted function, we can calculate the equivalent kernel at any given point. Two points
#   are calculated here, xa=0 and xb=0.6.
#   Epanechnikov kernel estimator for these two points are calculated based on the codes from HW1.
#   When xa=0, both equivalent and Epanechnikov kernels are scaled up by 12.
#   When xb=0.6, both kernels are scaled up by 20.
#   The blow-up aims at making the figures more readable and clearer. Choosing these two numbers are based
#   on trials which are to find out the better scaler.
#simulate data
set.seed(222)
x <- runif(100,0,1)
x <- x[order(x)]
x.vector <- as.matrix(x)
e <- rnorm(100,0,sqrt(1/3))
y <- sin(4*x) + e
y.vector <- as.matrix(y)
#bandwidth is 0.2
h <- 0.2
#Construct b matrix and x matrix
b <- matrix(NA,ncol=100,nrow=2)
for (i in 1:100) {
b[,i] <- t(matrix(c(1,x[i])))
}
x_mat <- t(b)
### Calculate W matrix
# First calculate Kh
t1 <- matrix(NA,nrow=100,ncol=100)
k <- matrix(NA,nrow=100,ncol=100)
for (i in 1:100) {
for (j in 1:100) {
t1[,j] <- x.vector - as.matrix(rep(x[j],100))
if (abs(t1[i,j]/h)>=1) {
t1[i,j] = h
} else {
t1[i,j] = t1[i,j]
}
k[i,j] <- 0.75*(1-(t1[i,j]/h)^2)/h
}
}
# Then construct a list with empty W matrices
mat <- matrix(NA, 100, 100)
w <- lapply(1:ncol(mat), function(i) replicate(100, mat[,i]))
# Finally assign Kh values to each empty W matrix
for (i in 1:100) {
w[[i]] <- diag(k[,i],100,100)
}
#gamma matrix
r <- matrix(NA,nrow=2,ncol=100)
for (i in 1:100) {
r[,i] <- (solve((t(x_mat)%*%w[[i]]%*%x_mat)))%*%
(t(x_mat)%*%w[[i]]%*%y.vector)
}
#fitted function
m <- matrix(NA,nrow=100,ncol=1)
for (i in 1:100) {
m[i,] <- t(b[,i])%*%r[,i]
}
#calculate equivalent kernel and Epanechnikov kernel given xa = 0
xa <- 0
ba <- t(matrix(c(1,xa)))
ta <- x.vector - as.matrix(rep(xa,100))
ka <- matrix(NA,nrow=100,ncol=1)
wa <- matrix(0,nrow=100,ncol=100)
# calculate Kh
for (i in 1:100) {
if (abs(ta[i,]/h)>=1) {
ta[i,] = h
} else {
ta[i,] = ta[i,]
}
ka[i,] <- 0.75*(1-(ta[i,]/h)^2)/h
wa[i,i] <- ka[i,]
}
#equivalent kernel is scalde up by 12
la <- ba%*%(solve(t(x_mat)%*%wa%*%x_mat)%*%t(x_mat)%*%wa)*12   #equivalent kernel
#Epanechnikov kernel is scaled up by 12
k_est <- ka/sum(ka)*12
ka_est <- k_est[k_est[,1] > 0]                            #Epanechnikov kernel
df <- cbind(x,y)
df_sub <- subset(df,df[,1]  >= (xa-h) & df[,1] <= (xa+h))
df_est <- cbind(df_sub,ka_est)
#Figure 5a
yticks <- seq(-2, 2.5, 0.5)
plot(x,y,xlim=c(0,1),ylim=c(-2,2.5),ylab='y',main='Figure 5a')
axis(2, at = yticks, labels = yticks)
polygon(x = c(0,df_est[,1]), y = c(0,df_est[,3]), density = NULL,
border = NA, col = "yellow", lty = par("lty"),
fillOddEven = FALSE)
lines(x,sin(4*x),col='green')
lines(x,m,col='black')
points(x,la,col='red')
legend("bottomleft",legend=c('y=sin(4x)','local linear'),
col=c('green','black'),lwd=1.5,cex=0.45)
### First simulate data based on y=sin(4x)+e.
### Bandwidth is set at 0.2.
### Construct b matrix [b(x)=t(1,x)] and X matrix [X = t(b(X1),b(X2), ..., b(Xn))]
### Calculate W matrix [diagonal matrix with diagonal vector t(Kh(X1-x0), ..., Kh(Xn-x0))]
### Calculate gamma matrix [(inverse(t(X)WX))*t(X)*W*t(Y1, ..., Yn)]
### Calculate fitted function [m(x)=t(b)*gamma]
### After obtained the fitted function, we can calculate the equivalent kernel at any given point. Two points
#   are calculated here, xa=0 and xb=0.6.
#   Epanechnikov kernel estimator for these two points are calculated based on the codes from HW1.
#   When xa=0, both equivalent and Epanechnikov kernels are scaled up by 12.
#   When xb=0.6, both kernels are scaled up by 20.
#   The blow-up aims at making the figures more readable and clearer. Choosing these two numbers are based
#   on trials which are to find out the better scaler.
#simulate data
set.seed(222)
x <- runif(100,0,1)
x <- x[order(x)]
x.vector <- as.matrix(x)
e <- rnorm(100,0,sqrt(1/3))
y <- sin(4*x) + e
y.vector <- as.matrix(y)
#bandwidth is 0.2
h <- 0.2
#Construct b matrix and x matrix
b <- matrix(NA,ncol=100,nrow=2)
for (i in 1:100) {
b[,i] <- t(matrix(c(1,x[i])))
}
x_mat <- t(b)
### Calculate W matrix
# First calculate Kh
t1 <- matrix(NA,nrow=100,ncol=100)
k <- matrix(NA,nrow=100,ncol=100)
for (i in 1:100) {
for (j in 1:100) {
t1[,j] <- x.vector - as.matrix(rep(x[j],100))
if (abs(t1[i,j]/h)>=1) {
t1[i,j] = h
} else {
t1[i,j] = t1[i,j]
}
k[i,j] <- 0.75*(1-(t1[i,j]/h)^2)/h
}
}
# Then construct a list with empty W matrices
mat <- matrix(NA, 100, 100)
w <- lapply(1:ncol(mat), function(i) replicate(100, mat[,i]))
# Finally assign Kh values to each empty W matrix
for (i in 1:100) {
w[[i]] <- diag(k[,i],100,100)
}
#gamma matrix
r <- matrix(NA,nrow=2,ncol=100)
for (i in 1:100) {
r[,i] <- (solve((t(x_mat)%*%w[[i]]%*%x_mat)))%*%
(t(x_mat)%*%w[[i]]%*%y.vector)
}
#fitted function
m <- matrix(NA,nrow=100,ncol=1)
for (i in 1:100) {
m[i,] <- t(b[,i])%*%r[,i]
}
#calculate equivalent kernel and Epanechnikov kernel given xa = 0
xa <- 0
ba <- t(matrix(c(1,xa)))
ta <- x.vector - as.matrix(rep(xa,100))
ka <- matrix(NA,nrow=100,ncol=1)
wa <- matrix(0,nrow=100,ncol=100)
# calculate Kh
for (i in 1:100) {
if (abs(ta[i,]/h)>=1) {
ta[i,] = h
} else {
ta[i,] = ta[i,]
}
ka[i,] <- 0.75*(1-(ta[i,]/h)^2)/h
wa[i,i] <- ka[i,]
}
#equivalent kernel is scalde up by 12
la <- ba%*%(solve(t(x_mat)%*%wa%*%x_mat)%*%t(x_mat)%*%wa)*12   #equivalent kernel
#Epanechnikov kernel is scaled up by 12
k_est <- ka/sum(ka)*12
ka_est <- k_est[k_est[,1] > 0]                            #Epanechnikov kernel
df <- cbind(x,y)
df_sub <- subset(df,df[,1]  >= (xa-h) & df[,1] <= (xa+h))
df_est <- cbind(df_sub,ka_est)
#Figure 5a
yticks <- seq(-2, 2.5, 0.5)
plot(x,y,xlim=c(0,1),ylim=c(-2,2.5),ylab='y',main='Figure 5a')
axis(2, at = yticks, labels = yticks)
polygon(x = c(0,df_est[,1]), y = c(0,df_est[,3]), density = NULL,
border = NA, col = "yellow", lty = par("lty"),
fillOddEven = FALSE)
lines(x,sin(4*x),col='green')
lines(x,m,col='black')
points(x,la,col='red')
legend("bottomleft",legend=c('y=sin(4x)','local linear'),
col=c('green','black'),lwd=1.5,cex=0.45)
#calculate equivalent kernel and Epanechnikov kernel given x0 = 0.6
xb <- 0.6
bb <- t(matrix(c(1,xb)))
tb <- x.vector - as.matrix(rep(xb,100))
kb <- matrix(NA,nrow=100,ncol=1)
wb <- matrix(0,nrow=100,ncol=100)
for (i in 1:100) {
if (abs(tb[i,]/h)>=1) {
tb[i,] = h
} else {
tb[i,] = tb[i,]
}
kb[i,] <- 0.75*(1-(tb[i,]/h)^2)/h
wb[i,i] <- kb[i,]
}
#equivalent kernel is scalde up by 20
lb <- bb%*%(solve(t(x_mat)%*%wb%*%x_mat)%*%t(x_mat)%*%wb)*20
#Epanechnikov kernel is scaled up by 20
k_est <- kb/sum(kb)*20
kb_est <- k_est[k_est[,1] > 0]
dfb_sub <- subset(df,df[,1]  >= (xb-h) & df[,1] <= (xb+h))
dfb_est <- cbind(dfb_sub,kb_est)
#Figure 5b
yticks <- seq(-2, 2.5, 0.5)
plot(x,y,xlim=c(0,1),ylim=c(-2,2.5),ylab='y',main='Figure 5b')
axis(2, at = yticks, labels = yticks)
polygon(x = dfb_est[,1], y = dfb_est[,3], density = NULL,
border = NA, col = "yellow", lty = par("lty"),
fillOddEven = FALSE)
lines(x,sin(4*x),col='green')
lines(x,m,col='black')
points(x,lb,col='red')
legend("topleft",legend=c('y=sin(4x)','local linear'),
col=c('green','black'),lwd=1.5,cex=0.45)
knitr::opts_chunk$set(echo = TRUE)
wd <- "CindyZhao$ /Users/CindyZhao/Desktop/machine\ learning/HW5/"
setwd(wd)
wd <- "/Users/CindyZhao/Desktop/machine\ learning/HW5/"
setwd(wd)
Sys.setlocale("LC_ALL", "C")
internetad_train <- read.csv("https://www.dropbox.com/s/soi7a4o6ny8ahhm/internetads_train.csv?dl=1",as.is=T)
internetad_test <- read.csv("https://www.dropbox.com/s/0i5cy1nf2612h8m/internetads_test.csv?dl=1",as.is=T)
v <- readRDS("varImp.rds")
wd <- "/Users/CindyZhao/Desktop/machine learning/HW5/"
setwd(wd)
v <- readRDS("varImp.rds")
wd <- "/Users/CindyZhao/Desktop/machine learning/HW5/"
setwd(wd)
v <- readRDS("varImp.rds")
wd <- "/Users/CindyZhao/Desktop/machine learning/HW5/"
setwd(wd)
data(iris)
my_lm_summary = function(Y, X) {
#calculate beta
beta <- solve((t(X) %*% X)) %*% t(X) %*% Y
#calculate standard errors of residuals
Y_fit <- X %*% beta
Y <- as.matrix(Y)
n <- nrow(X)
sigma_sq <- sum((Y-Y_fit)^2)/(n - ncol(X))
sigma_sq <- diag(x=sigma_sq,nrow=ncol(X),ncol=ncol(X))
#calculate stantard errors of beta
se_beta <- as.matrix(sqrt(diag(sigma_sq * solve((t(X) %*% X)))))
#calculate t value and corresponding p value
tval <- matrix(NA,3,1)
pval <- matrix(NA,3,1)
for (i in 1:3) {
tval[i,1] <- beta[i,1]/se_beta[i,1]
pval[i,1] <- 2*pt(-abs(tval[i,1]),df=n-ncol(X))
}
result <- cbind(beta,se_beta,tval,pval)
colnames(result) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
return(result)
}
X = model.matrix(~ Sepal.Width + Petal.Length, data=iris)
Y = iris$Sepal.Length
print(my_lm_summary(Y, X))
class(result)
class(my_lm_summary)
result
X <- model.matrix(~ Sepal.Width + Petal.Length, data=iris)
Y = iris$Sepal.Length
beta <- solve((t(X) %*% X)) %*% t(X) %*% Y
#calculate standard errors of residuals
Y_fit <- X %*% beta
Y <- as.matrix(Y)
n <- nrow(X)
sigma_sq <- sum((Y-Y_fit)^2)/(n - ncol(X))
sigma_sq <- diag(x=sigma_sq,nrow=ncol(X),ncol=ncol(X))
#calculate stantard errors of beta
se_beta <- as.matrix(sqrt(diag(sigma_sq * solve((t(X) %*% X)))))
#calculate t value and corresponding p value
tval <- matrix(NA,3,1)
pval <- matrix(NA,3,1)
for (i in 1:3) {
tval[i,1] <- beta[i,1]/se_beta[i,1]
pval[i,1] <- 2*pt(-abs(tval[i,1]),df=n-ncol(X))
}
result <- cbind(beta,se_beta,tval,pval)
colnames(result) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
return(result)
X <- model.matrix(~ Sepal.Width + Petal.Length, data=iris)
Y = iris$Sepal.Length
beta <- solve((t(X) %*% X)) %*% t(X) %*% Y
#calculate standard errors of residuals
Y_fit <- X %*% beta
Y <- as.matrix(Y)
n <- nrow(X)
sigma_sq <- sum((Y-Y_fit)^2)/(n - ncol(X))
sigma_sq <- diag(x=sigma_sq,nrow=ncol(X),ncol=ncol(X))
#calculate stantard errors of beta
se_beta <- as.matrix(sqrt(diag(sigma_sq * solve((t(X) %*% X)))))
#calculate t value and corresponding p value
tval <- matrix(NA,3,1)
pval <- matrix(NA,3,1)
for (i in 1:3) {
tval[i,1] <- beta[i,1]/se_beta[i,1]
pval[i,1] <- 2*pt(-abs(tval[i,1]),df=n-ncol(X))
}
result <- cbind(beta,se_beta,tval,pval)
colnames(result) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
result
class(result)
class(result) <- c("lm")
result
print(result)
result[]
print(result[])
data(iris)
my_lm_summary = function(Y, X) {
#calculate beta
#X <- model.matrix(~ Sepal.Width + Petal.Length, data=iris)
#Y = iris$Sepal.Length
beta <- solve((t(X) %*% X)) %*% t(X) %*% Y
#calculate standard errors of residuals
Y_fit <- X %*% beta
Y <- as.matrix(Y)
n <- nrow(X)
sigma_sq <- sum((Y-Y_fit)^2)/(n - ncol(X))
sigma_sq <- diag(x=sigma_sq,nrow=ncol(X),ncol=ncol(X))
#calculate stantard errors of beta
se_beta <- as.matrix(sqrt(diag(sigma_sq * solve((t(X) %*% X)))))
#calculate t value and corresponding p value
tval <- matrix(NA,3,1)
pval <- matrix(NA,3,1)
for (i in 1:3) {
tval[i,1] <- beta[i,1]/se_beta[i,1]
pval[i,1] <- 2*pt(-abs(tval[i,1]),df=n-ncol(X))
}
result <- cbind(beta,se_beta,tval,pval)
colnames(result) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
class(result) <- c("lm")
return(result[])
}
X = model.matrix(~ Sepal.Width + Petal.Length, data=iris)
Y = iris$Sepal.Length
print(my_lm_summary(Y, X))
#' Fit a linear model
#'
#' @description This function passes parameters to the lm function.
#' @param formula a formula
#' @param data a data.frame
#' @return An lm object
#' @examples
#' fit <- linear_model(Sepal.Length ~., iris)
#' summary(fit)
#' @export
linear_model <- function(formula, data) {
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
return(beta[])
}
library(testthat)
context("Test the output of homework 1.")
test_that("The updated version of linear_model works.", {
data(lm_patho)
fit_linear_model <- linear_model(y ~., lm_patho)
fit_lm <- lm(y ~., lm_patho)
expect_equivalent(fit_lm$coefficients, fit_linear_model$coefficients)
})
devtools::test()
#' Fit a linear model
#'
#' @description This function passes parameters to the lm function.
#' @param formula a formula
#' @param data a data.frame
#' @return An lm object
#' @examples
#' fit <- linear_model(Sepal.Length ~., iris)
#' summary(fit)
#' @export
linear_model <- function(formula, data) {
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
return(beta[])
}
devtools::test()
library(testthat)
context("Test the output of homework 1.")
test_that("The updated version of linear_model works.", {
data(lm_patho)
fit_linear_model <- linear_model(y ~., lm_patho)
fit_lm <- lm(y ~., lm_patho)
expect_equivalent(fit_lm$coefficients, fit_linear_model$coefficients)
})
linear_model <- function(formula, data) {
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
y_fit <- xm %*% beta
y_fit <- as.matrix(ym)
n <- nrow(xm)
sigma_sq <- sum((ym - y_fit)^2)/(n - ncol(xm))
sigma_sq <- diag(x=sigma_sq, nrow = ncol(xm), ncol = ncol(xm))
se_beta <- as.matrix(sqrt(diag(sigma_sq * solve((t(xm) %*% xm)))))
tval <- matrix(NA, 3, 1)
pval <- matrix(NA, 3, 1)
for (i in 1:3) {
tval[i,1] <- beta[i,1]/se_beta[i,1]
pval[i,1] <- 2*pt(-abs(tval[i,1]),df=n - ncol(xm))
}
result <- cbind(beta,se_beta,tval,pval)
colnames(result) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")
class(result) <- c("lm")
return(result[])
}
linear_model(Sepal.Length ~., iris)
#' Fit a linear model
#'
#' @description This function passes parameters to the lm function.
#' @param formula a formula
#' @param data a data.frame
#' @return An lm object
#' @examples
#' fit <- linear_model(Sepal.Length ~., iris)
#' summary(fit)
#' @export
linear_model <- function(formula, data) {
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
return(beta[])
}
linear_model(Sepal.Length ~., iris)
formula <- Sepal.length ~ .
data <- iris
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
iris$Sepal.Length
formula <- Sepal.Length ~ .
data <- iris
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
beta[]
class(beta)
formula <- Sepal.Length ~ .
data <- iris
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
colname(beta) <- "coeff"
formula <- Sepal.Length ~ .
data <- iris
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
colnames(beta) <- "coeff"
beta[]
linear_model <- function(formula, data) {
xm <- model.matrix(formula,data)
yname <- paste(as.list(formula)[[2]])
ym <- as.matrix(data[,yname])
beta <- solve(t(xm)%*%xm)%*%t(xm)%*%ym
class(beta) <- c("lm")
colnames(beta) <- "coefficient"
return(beta[])
}
linear_model(Sepal.Length~., iris)
devtools::test()
devtools::use_vignette("my-vignette")
setwd("/Users/CindyZhao/Documents/GitHub/bis557/vignettes/")
devtools::use_vignette("my-vignette")
devtools::use_vignette("my-vignette")
