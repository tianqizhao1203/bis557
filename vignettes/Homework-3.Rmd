---
title: "Homework-3"
author: "Tianqi Zhao"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  fig.width=7, 
  fig.height=5,
  collapse = TRUE,
  comment = "#>"
)
```

# Question 7
Kernels can also be used as density estimators. Specifically, we have
        $f_h(x) = \frac{1}{n} \sum_i K_h(x - x_i)$

In this setting we see again why it is important to have the integral of the kernel equal to 1. Write a function kern_density that accepts a training vector $x$, bandwidth $h$, and test set x_new, returning the kernel density estimate from the Epanechnikov kernel. Visually test how this performs for some hand constructed datasets and bandwidths.

```{r}
kern_density <- function(x, h, x_new){
  ind <- matrix(NA, length(x), length(x_new))
  kh <- matrix(NA, length(x), length(x_new))
  fh <- NULL
  for (i in 1:length(x)){
    for (j in 1:length(x_new)){
      ind[i,j] <- ifelse(abs((x[i] - x_new[j])/h) <= 1, 1, 0)
      # Calculate Kh
      kh[i,j] <- 3/4 * (1 - ((x[i] - x_new[j])/h)^2)/h * ind[i,j]
      # Calculate density
      fh[j] <- 1/length(x) * sum(kh[,j])
    }
  }
  return(fh)
}

set.seed(2233)
x <- rnorm(1000,0,1)
x <- as.vector(x[order(x)])
x_new <- rnorm(50,0,1)
x_new <- as.vector(x_new[order(x_new)])

# Compare the kernel estimation to the true normal density
par(mfrow = c(1, 3))
plot(x_new, kern_density(x, 0.2, x_new), ylim = c(0, 0.45), ylab = "Kernel Density", xlab = "x_new", type = "l", main = "Bandwidth = 0.2")
lines(x_new, dnorm(x_new), col = "red")
legend("bottom",legend = c('Density for Normal','Kernel Density'),
       col=c('red','black'),lwd = 1.5,cex = 0.65)


plot(x_new, kern_density(x, 0.5, x_new), ylim = c(0, 0.45), ylab = "Kernel Density", xlab = "x_new", type = "l", main = "Bandwidth = 0.5")
lines(x_new, dnorm(x_new), col = "red")
legend("bottom",legend = c('Density for Normal','Kernel Density'),
       col=c('red','black'),lwd = 1.5,cex = 0.65)

plot(x_new, kern_density(x, 1, x_new), ylim = c(0, 0.45), ylab = "Kernel Density", xlab = "x_new", type = "l", main = "Bandwidth = 1")
lines(x_new, dnorm(x_new), col = "red")
legend("bottom",legend = c('Density for Normal','Kernel Density'),
       col=c('red','black'),lwd = 1.5,cex = 0.65)
```

# Question 3
Show that if f and g are both convex functions, then their sum must also be convex.

## Proof:
Based on the definition of convex function, 
$$
\begin{aligned}
& f(\lambda x + (1 - \lambda)y) \leqslant \lambda f(x) + (1 - \lambda)f(y)\\
& g(\lambda x + (1 - \lambda)y) \leqslant \lambda g(x) + (1 - \lambda)g(y)\\
& where\  \lambda \in [0, 1]
\end{aligned}
$$
Suppose $h(x) = f(x) + g(x)$. Then
$$
\begin{aligned}
h(\lambda x + (1 - \lambda)y) &= f(\lambda x + (1 - \lambda)y) + g(\lambda x + (1 - \lambda)y)\\
&\leqslant \lambda f(x) + (1 - \lambda)f(y) + \lambda g(x) + (1 - \lambda)g(y)\\
&=\lambda (f(x) + g(x)) + (1 - \lambda)(f(y) + g(y))\\
&=\lambda h(x) + (1 - \lambda) h(y)
\end{aligned}
$$
Therefore, $h = f + g$ is convex.

# Question 4
Illustrate that the absolute value function is convex. Using the result from the previous exercise, show that the $l_1$-norm is also convex.

## Proof
Suppose $x, y \in \mathbb{R}$ and $f(x) = |x|$.
$$
\begin{aligned}
f(\lambda x + (1 - \lambda)y) &= |\lambda x + (1 - \lambda)y| \ \ \ where\ \lambda \in [0,1]\\
&\leqslant |\lambda x| + |(1 - \lambda)y| \\
&=\lambda |x| + (1 - \lambda) |y| \\
&=\lambda f(x) + (1 - \lambda) f(y)
\end{aligned}
$$
Therefore, $f(x) = |x|$ is a convex function.

Because $l_1-norm = \sum_{i = 1}|v_i|$ and $|v_i|$ is convex, then $l_1$-norm is convex.

# Question 5
Prove that the elastic net objective function is convex using the results from the previous two exercises.

## Proof
Based on the formula of elastic net, we know that it is the sum of squared $l_2$-norms and $l_1$-norm.

Suppose $f(x) = x^2$ and $x, y \in \mathbb{R}$.
$$
\begin{aligned}
&When\ \lambda \in [0,1],\\
&(\lambda^2 - \lambda)(x - y)^2 \leqslant 0 \\
&\lambda^2x^2 - \lambda x^2 + 2\lambda (1 - \lambda)xy + \lambda^2y^2 - \lambda y^2 \leqslant 0 \\
&\lambda^2x^2 + 2\lambda(1-\lambda)xy + y^2 - 2\lambda y^2 + \lambda^2y^2 \leqslant \lambda x^2 + y^2 - \lambda y^2 \\
&\lambda^2x^2 + 2\lambda(1 - \lambda)xy + (1 - \lambda)^2y^2 \leqslant \lambda x^2 + (1 - \lambda)y^2 \\
&(\lambda x + (1 - \lambda)y)^2 \leqslant \lambda x^2 + (1 - \lambda)y^2 \\
&f(\lambda x + (1 - \lambda)y) \leqslant \lambda f(x) + (1 - \lambda)f(y)
\end{aligned}
$$
Therefore, $f(x) = x^2$ is convex. Squared $l_2$-norm is the sum of $x_i^2$, so $l_2$-norm is convex. The elastic net is the sum of $l_1$-norm and squared $l_2$-norms, both of which are convex. So the elastic net is also convex.

# Question 6
Find the KKT conditions for glmnet when $\alpha = 1$ and implement a lasso_reg_with_screening function that takes an $\alpha$ parameter.

## Solution:
When $\alpha = 1$, glmnet becomes lasso regression.

KKT conditions for lasso regression is
$$
\begin{aligned}
&\frac{1}{n}\sum_{i=1}^{n}x_{il}(y_i-\sum_{j=1}^{p}x_{ij}\hat b_j)=\lambda s_l \\
&where \\
&s_l \in \left\{\begin{array}{l}1 \ \ \ \ \ \ \ \ \ \ \ \ \ \ if \ \hat b_l>0 \\ -1 \ \ \ \ \ \ \ \ \ \ \ if \ \hat b_l<0 \\ [-1, 1] \ \ \ \ \ if \ \hat b_l=0 \end{array}\right.
\end{aligned}
$$
```{r}
library(glmnet)
library(ISLR)
lasso_reg_with_screening <- function(X, y, b, lambda){
  resids <- y - X %*% b
  s <- apply(X, 2, function(xj) crossprod(xj, resids)) / lambda / nrow(X)
  result <- (b == 0) & (abs(s) >= 1)
  count = 0
  for (i in 1:length(result)) {
      if (result[i] == TRUE) {
      count = count + 1
      }
  }
  return(list(result, count/length(result)))
}

# Implementation
# Use the 1-SE rule to choose lambda
X1 <- scale(model.matrix(Sepal.Length ~. -1, iris))
y1 <- iris$Sepal.Length - mean(iris$Sepal.Length)
cv.lasso1 <- cv.glmnet(X1, y1, alpha=1)
lambda1 <- cv.lasso1$lambda.1se
b1 <- cv.lasso1$glmnet.fit$beta[, cv.lasso1$lambda == lambda1]
lasso_reg_with_screening(X1, y1, b1, lambda1)

Hitters=na.omit(Hitters)
X2 <- scale(model.matrix(Salary ~ . -1, Hitters))
y2 <- Hitters$Salary - mean(Hitters$Salary)
cv.lasso2 <- cv.glmnet(X2, y2, alpha=1)
lambda2 <- cv.lasso2$lambda.1se
b2 <- cv.lasso2$glmnet.fit$beta[, cv.lasso2$lambda == lambda2]
lasso_reg_with_screening(X2, y2, b2, lambda2)
```

Based on the KKT conditions for lasso regression, the lasso_reg_with_screening was created. The returned list of this function contains two parts; the first part is whether the KKT conditions are violated ('False' means no violation), and the second part is the proportion of coefficients that breaks the KKT conditions.

I used two datasets for testing when the lambda used was $lambda.1se$. None of the coefficient estimations from glmnet broke the KKT conditions. 
