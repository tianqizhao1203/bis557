---
title: "Homework-5"
author: "Tianqi Zhao"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Question 1
In class we used the LASSO to predict hand-writting characters in the MNIST data set. Increase the
out-of-sample prediction accuracy by extracting predictive features from the images.

```{r}
library(keras)
library(glmnet)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

set.seed(123)
s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")

ose <- matrix(NA, length(fit$lambda), 2)
for (i in 1:length(fit$lambda)) {
  preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda[i], type = "class")
  t <- table(as.vector(preds), y_test)
  ose[i,1] <- sum(diag(t)) / sum(t)
  ose[i,2] <- fit$lambda[i]
}

ose[which.max(ose[,1]),]
# The out-of-sample accuracy is the highest (0.854) when lambda = 0.002109465

#preds <- predict(fit$glmnet.fit, x_test, s = 0.0008, type = "class")
#t <- table(as.vector(preds), y_test)
#sum(diag(t)) / sum(t)
#preds <- predict(fit$glmnet.fit, x_test, s = 0.0005, type = "class")
#t <- table(as.vector(preds), y_test)
#sum(diag(t)) / sum(t)
```

The out-of-sample accuracy is the highest (0.854) when lambda = 0.002109465. But the accuracy is still not high. 

Intuitively, to improve the prediction accurary, we can decrease lambda to include more predictors. However, after several attemps, there is still no obvious improvement in Lasso. Probably, around 0.85 is almost the best that Lasso can do. But following in Q2, we can see that CNN can significantly increase the prediction accuracy from 0.85 to 0.99.


# Question 2 (263)
Adjust the kernel size, and any other parameters you think are useful, in the convolutional neural network for EMNIST in Section 8.10.4. Can you improve on the classification rate?

```{r}
# EMNIST is too large to read in
# Prof. Kane mentioned that we can just use MNIST dataset.
# Codes from textbook and lectures
# Transform RGB values into [0,1] range
x_train <- (x_train / 255) 
x_test <- (x_test / 255)

# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train,26L)
y_test <- to_categorical(y_test,26L)
X_train <- array_reshape(as.matrix(x_train), dim = c(nrow(x_train), 28, 28, 1))
X_valid <- array_reshape(as.matrix(x_test), dim = c(nrow(x_test), 28, 28, 1))
Y_train <- y_train
Y_valid <- y_test

# Build model
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                input_shape = c(28, 28, 1),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%

  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

# Compile model
model %>% compile(loss = 'categorical_crossentropy',
                    optimizer = optimizer_rmsprop(),
                    metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 10, validation_data = list(X_valid, Y_valid)) # highest val_acc: 0.9878

# Prediction
predict_train <- predict_classes(model, X_train)
predict_test <- predict_classes(model, X_valid)

# Change y_train and y_test back to 10 classes
y_class_train <- rep(NA, nrow(y_train))
for (i in 1:26) {
  for (j in 1:nrow(y_train)) {
    if (y_train[j,i] != 0) {
      y_class_train[j] <- i - 1
    }
  }
}

y_class_test <- rep(NA, nrow(y_test))
for (i in 1:26) {
  for (j in 1:nrow(y_test)) {
    if (y_test[j,i] != 0) {
      y_class_test[j] <- i - 1
    }
  }
}

# Prediction accuracy
sum(predict_train == y_class_train)/length(y_class_train)
sum(predict_test == y_class_test)/length(y_class_test)

```

Before adjustments, the prediction accuracy in the train set is 0.9851 and in the validation set is 0.9871.

If adjust kernel size to c(5, 5), then the prediction accuracy in train set is 0.9838 and in validation set is 0.9832.

If adjust drop-out rate to 0.25, then the prediction accuracy in train set is 0.9929 and in validation set is 0.9894.

If adjust both, then the prediction accuract in train set is 0.9925 and in validation set is 0.9907.

For the number of epochs, based on the plot for history, only when epoch = 1, the prediction accuracy is relatively low and when epoch >= 2, the accuracy tends to become stable. Therefore, the number of epochs is not tuned here.
Because the accuracy of predictin is already high before any adjustment, the improvement is not that big. It seems that drop-out rate is more effective in improving the accuracy than kernel size. But the combination effects of them are better.

```{r}
# Build a keras model 
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                input_shape = c(28, 28, 1),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%

  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                    optimizer = optimizer_rmsprop(),
                    metrics = c('accuracy'))
history <- model %>% 
  fit(X_train, Y_train, epochs = 10, validation_data = list(X_valid, Y_valid))

# Prediction
predict_train <- predict_classes(model, X_train)
predict_test <- predict_classes(model, X_valid)

# Change y_train and y_test back to 10 classes
y_class_train <- rep(NA, nrow(y_train))
for (i in 1:26) {
  for (j in 1:nrow(y_train)) {
    if (y_train[j,i] != 0) {
      y_class_train[j] <- i - 1
    }
  }
}

y_class_test <- rep(NA, nrow(y_test))
for (i in 1:26) {
  for (j in 1:nrow(y_test)) {
    if (y_test[j,i] != 0) {
      y_class_test[j] <- i - 1
    }
  }
}

# Prediction accuracy
sum(predict_train == y_class_train)/length(y_class_train)
sum(predict_test == y_class_test)/length(y_class_test)
```


# Question 3
Write a function that uses mean absolute deviation as a loss function, instead of mean squared error. Test the use of this function with a simulation containing several outliers. How well do neural networks and SGD perform when using robust techniques?

```{r}
# Functions from textbook
# Create list of weights to describe a dense neural network.
casl_nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]), ncol = sizes[j], nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w, b=rnorm(sizes[j + 1L]))
  }
  weights
}

# Apply a rectified linear unit (ReLU) to a vector/matrix.
casl_util_ReLU <- function(v) {
  v[v < 0] <- 0
  v
}

# Apply derivative of the rectified linear unit (ReLU).
casl_util_ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}

# Derivative of the MSE function.
casl_util_mse_p <- function(y, a) {
  (a - y)
}

# Apply forward propagation to a set of NN weights and biases.
casl_nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}

# Apply backward propagation algorithm.
casl_nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z; a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
    } else {
      grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * sigma_p(z[[j]])
      }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}

# Apply stochastic gradient descent (SGD) to estimate NN.
casl_nn_sgd <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
  }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights, f_obj, casl_util_ReLU_p, casl_util_mse_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}

# Predict values from a training neural network.
casl_nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
  }
  y_hat
}
```

```{r}
# Change loss function
# Derivative of the mean absolute deviation function.
casl_util_mad_p <- function(y, a) {
  (a - y)/abs(a - y)
}

# Apply stochastic gradient descent (SGD) to estimate NN (using MAD).
casl_nn_sgd_mad <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
  }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights, f_obj, casl_util_ReLU_p, casl_util_mad_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}

set.seed(123)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
y[100, 1] <- 100
y[200, 1] <- -20
y[300, 1] <- 65
weights1 <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
y_pred1 <- casl_nn_predict(weights1, X)
weights2 <- casl_nn_sgd_mad(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
y_pred2 <- casl_nn_predict(weights2, X)

# Compare y with predictions
plot(y, main = "MSE Loss Function")
points(y_pred1, col = "red")

plot(y, main = "MAD Loss Function")
points(y_pred2, col = "blue")

# MSE
mean((y-y_pred1)^2) #14.56506
mean((y-y_pred2)^2) #14.56829

# MAD
mean(abs(y-y_pred1)) #0.2926986
mean(abs(y-y_pred2)) #0.3021683

# Outliers
c(y_pred1[100], y_pred1[200], y_pred1[300])
c(y_pred2[100], y_pred2[200], y_pred2[300])
```

From the plots, we can see that the predictions are not affected by outliers with both MSE and MAD loss functions. Neural networks and SGD perform well when using robust techniques. Also from the results of MSE and MAD between predictions and true values, the values are really close. And the predicted values of outliers are also similar. So the effects of outliers on both methods are not that big, and both methods are robust enough.

```{r}
# Perform a gradient check for the dense NN code.
casl_nn_grad_check <- function(X, y, weights, h=0.0001) {
  max_diff <- 0
  for (level in seq_along(weights)) {
    for (id in seq_along(weights[[level]]$w)) {
      grad <- rep(0, nrow(X))
      for (i in seq_len(nrow(X))) {
        f_obj <- casl_nn_forward_prop(X[i, ], weights,casl_util_ReLU)
        b_obj <- casl_nn_backward_prop(X[i, ], y[i, ], weights, f_obj, casl_util_ReLU_p,
casl_util_mse_p)
        grad[i] <- b_obj$grad_w[[level]][id]
      }
      w2 <- weights
      w2[[level]]$w[id] <- w2[[level]]$w[id] + h
      f_h_plus <- 0.5 * (casl_nn_predict(w2, X) - y)^2
      w2[[level]]$w[id] <- w2[[level]]$w[id] - 2 * h
      f_h_minus <- 0.5 * (casl_nn_predict(w2, X) - y)^2
      grad_emp <- sum((f_h_plus - f_h_minus) / (2 * h))
      max_diff <- max(max_diff, abs(sum(grad) - grad_emp))
    }
  }
  max_diff
}
# MSE loss function
weights <- casl_nn_sgd(X, y, sizes=c(1, 5, 5, 1), epochs=1, eta=0.01)
casl_nn_grad_check(X, y, weights) # 5.685592e-09

# MAD loss function
weights <- casl_nn_sgd_mad(X, y, sizes=c(1, 5, 5, 1), epochs=1, eta=0.01)
casl_nn_grad_check(X, y, weights) # 4.910817e-09
```

Both numeric gradients match all of the simulated gradients up to a very small number, indicating that the computations of the gradient based on two different loss functions are accurate.